<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>AmiVoice API Speech Recognition Sample</title>
    <script type="text/javascript" src="./scripts/opus-encoder-wrapper.js"></script>
    <script type="text/javascript" src="./scripts/ami-asynchrp.js"></script>
    <script type="text/javascript" src="./scripts/ami-easy-hrp.js"></script>
    <script type="text/javascript" src="./lib/wrp/recorder.js"></script>
    <script type="text/javascript" src="./lib/wrp/wrp.js"></script>
    <style>
        * {
            font-family: 'Hiragino Kaku Gothic ProN', 'Helvetica', 'Verdana', 'Lucida Grande', sans-serif;
        }

        a {
            margin-right: 5px;
        }

        button {
            margin: 5px;
        }

        video {
            background: black;
            width: 600px;
        }
        
        #latencyInfo {
            background: #f5f5f5;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            margin: 10px 0;
            font-family: monospace;
        }
        
        #latencyInfo p {
            margin: 5px 0;
            font-size: 14px;
        }
        
        #latencyInfo span {
            font-weight: bold;
            color: #0066cc;
        }
    </style>
</head>

<body>
    <table>
        <tbody>
            <tr>
                <td><label for="appKey">APPKEY</label></td>
                <td><input id="appKey" type="password"><br></td>
            </tr>
            <tr>
                <td><label for="audioFile">Audio File</label></td>
                <td><input id="audioFile" type="file" accept=".wav,.mp3,.flac,.opus,.m4a,.mp4,.webm"></td>
            </tr>
            <tr>
                <td><label>Audio File Information</label></td>
                <td><span id="audioInfo"></span></td>
            </tr>
            <tr>
                <td><label for="engineMode">Connection Engine<label></td>
                <td>
                    <select name="engineMode" id="engineMode">
                        <option value="-a-general">Conversation_General</option>
                        <option value="-a-general-input">Voice Input_General</option>
                        <option value="-a-medgeneral">Conversation_Medical</option>
                        <option value="-a-medgeneral-input">Voice Input_Medical</option>
                        <option value="-a-bizmrreport">Conversation_Pharmaceutical</option>
                        <option value="-a-bizmrreport-input">Voice Input_Pharmaceutical</option>
                        <option value="-a-medkarte-input">Voice Input_Electronic Medical Record</option>
                        <option value="-a-bizinsurance">Conversation_Insurance</option>
                        <option value="-a-bizinsurance-input">Voice Input_Insurance</option>
                        <option value="-a-bizfinance">Conversation_Finance</option>
                        <option value="-a-bizfinance-input">Voice Input_Finance</option>
                        <option value="-a-general-en">English_General</option>
                        <option value="-a-general-zh">Chinese_General</option>
                    </select>
                </td>
            </tr>
            <tr>
                <td><label for="loggingOptOut">Do not provide audio and recognition results for service improvement (no log storage)</label></td>
                <td><input type="checkbox" id="loggingOptOut" checked></td>
            </tr>
            <tr>
                <td><label for="keepFillerToken">Include filler words (hesitations) in recognition results</label></td>
                <td><input type="checkbox" id="keepFillerToken"></td>
            </tr>
            <tr>
                <td><label for="speakerDiarization">Enable speaker diarization</label></td>
                <td><input type="checkbox" id="speakerDiarization"></td>
            </tr>
            <tr>
                <td><label for="sentimentAnalysis">Enable sentiment analysis (Asynchronous HTTP Speech Recognition API only)</label></td>
                <td><input type="checkbox" id="sentimentAnalysis"></td>
            </tr>
            <tr>
                <td><label for="profileWords">User Registered Words</label></td>
                <td><input type="text" id="profileWords"
                        title="Specify as {notation1}{space}{reading1}|{notation2}{space}{reading2}. Example: AmiVoice amivoisu|neko neko"></td>
            </tr>
            <tr>
                <td><label for="useUserMedia">Recognize microphone audio (for WebSocket Speech Recognition API)</label></td>
                <td><input type="checkbox" id="useUserMedia" checked></td>
            </tr>
            <tr>
                <td><label for="useDisplayMedia">Recognize system audio (for WebSocket Speech Recognition API)</label></td>
                <td><input type="checkbox" id="useDisplayMedia"></td>
            </tr>
            <tr>
                <td><label for="useOpusRecorder">Compress audio data to Ogg Opus format before sending to server</label></td>
                <td><input type="checkbox" id="useOpusRecorder" checked></td>
            </tr>
            <tr>
                <td colspan="2">
                    <div style="font-size: smaller; margin-left: 20px;">
                        <div>The following program is used for compression to Ogg Opus format.</div>
                        <div>
                            Opus Recorder License (MIT)<br>
                            Original Work Copyright © 2013 Matt Diamond<br>
                            Modified Work Copyright © 2014 Christopher Rudmin<br>
                            <a href="https://github.com/chris-rudmin/opus-recorder/blob/v8.0.5/LICENSE.md"
                                target="_blank"
                                rel="noopener noreferrer">https://github.com/chris-rudmin/opus-recorder/blob/v8.0.5/LICENSE.md</a>
                        </div>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
    <div>
        <a href="player.html" rel="noopener noreferrer" target="_blank">Open Audio Player</a>
        <div>
            <button id="executeAsyncButton">Execute Asynchronous HTTP Speech Recognition API (Audio File)</button><br>
            <button id="startWrpButton">Start WebSocket Speech Recognition API (Microphone or System)</button>
            <button id="stopWrpButton">Stop WebSocket Speech Recognition API</button><br>
            <button id="executeHrpButton">Execute Synchronous HTTP Speech Recognition API (Short Audio File)</button>
        </div>
        <div>
            <h3>Latency Information</h3>
            <div id="latencyInfo">
                <p>Last Audio Sent: <span id="lastAudioSent">-</span></p>
                <p>Last Result Received: <span id="lastResultReceived">-</span></p>
                <p>Current Latency: <span id="currentLatency">-</span></p>
                <p>Average Latency: <span id="averageLatency">-</span></p>
                <p>Min Latency: <span id="minLatency">-</span></p>
                <p>Max Latency: <span id="maxLatency">-</span></p>
            </div>
        </div>
        <div>
            <textarea id="logs" readonly></textarea>
        </div>
        <script>
            (function () {
                const appKeyElement = document.getElementById("appKey");
                const audioFileElement = document.getElementById("audioFile");
                const audioInfoElement = document.getElementById("audioInfo");
                const engineModeElement = document.getElementById("engineMode");
                const loggingOptOutElement = document.getElementById("loggingOptOut");
                const keepFillerTokenElement = document.getElementById("keepFillerToken");
                const speakerDiarizationElement = document.getElementById("speakerDiarization");
                const sentimentAnalysisElement = document.getElementById("sentimentAnalysis");
                const profileWordsElement = document.getElementById("profileWords");
                const useUserMediaElement = document.getElementById("useUserMedia");
                const useDisplayMediaElement = document.getElementById("useDisplayMedia");
                const useOpusRecorderElement = document.getElementById("useOpusRecorder");

                const executeAsyncButtonElement = document.getElementById("executeAsyncButton");
                const startWrpButtonElement = document.getElementById("startWrpButton");
                const stopWrpButtonElement = document.getElementById("stopWrpButton");
                const executeHrpButtonElement = document.getElementById("executeHrpButton");

                const logsElement = document.getElementById("logs");

                // Latency tracking variables
                const lastAudioSentElement = document.getElementById("lastAudioSent");
                const lastResultReceivedElement = document.getElementById("lastResultReceived");
                const currentLatencyElement = document.getElementById("currentLatency");
                const averageLatencyElement = document.getElementById("averageLatency");
                const minLatencyElement = document.getElementById("minLatency");
                const maxLatencyElement = document.getElementById("maxLatency");
                
                let lastAudioSentTime = null;
                let latencyHistory = [];
                let audioChunkCounter = 0;
                let resultCounter = 0;

                // Latency tracking functions
                function updateLatencyInfo() {
                    if (lastAudioSentTime) {
                        lastAudioSentElement.textContent = new Date(lastAudioSentTime).toLocaleTimeString();
                    }
                    if (latencyHistory.length > 0) {
                        const latest = latencyHistory[latencyHistory.length - 1];
                        currentLatencyElement.textContent = latest.toFixed(2) + " ms";
                        averageLatencyElement.textContent = (latencyHistory.reduce((a, b) => a + b, 0) / latencyHistory.length).toFixed(2) + " ms";
                        minLatencyElement.textContent = Math.min(...latencyHistory).toFixed(2) + " ms";
                        maxLatencyElement.textContent = Math.max(...latencyHistory).toFixed(2) + " ms";
                    }
                }

                function recordAudioSent() {
                    lastAudioSentTime = Date.now();
                    audioChunkCounter++;
                    updateLatencyInfo();
                }

                function recordResultReceived() {
                    if (lastAudioSentTime) {
                        const latency = Date.now() - lastAudioSentTime;
                        latencyHistory.push(latency);
                        resultCounter++;
                        
                        // Keep only last 100 measurements for performance
                        if (latencyHistory.length > 100) {
                            latencyHistory.shift();
                        }
                        
                        lastResultReceivedElement.textContent = new Date().toLocaleTimeString();
                        updateLatencyInfo();
                        
                        addLog(`Latency: ${latency.toFixed(2)}ms (Audio chunk: ${audioChunkCounter}, Result: ${resultCounter})`);
                    }
                }

                // Custom WebSocket wrapper to track audio data transmission
                function createLatencyTrackingWebSocket() {
                    // Store original WebSocket
                    const OriginalWebSocket = window.WebSocket;
                    
                    // Override WebSocket constructor
                    window.WebSocket = function(url, protocols) {
                        const ws = new OriginalWebSocket(url, protocols);
                        
                        // Override send method to track audio data
                        const originalSend = ws.send;
                        ws.send = function(data) {
                            // Check if this is audio data (starts with 'p' character)
                            if (data instanceof Uint8Array && data.length > 0 && data[0] === 0x70) {
                                recordAudioSent();
                            }
                            return originalSend.call(this, data);
                        };
                        
                        return ws;
                    };
                    
                    // Copy static properties
                    Object.setPrototypeOf(window.WebSocket, OriginalWebSocket);
                    Object.setPrototypeOf(window.WebSocket.prototype, OriginalWebSocket.prototype);
                }

                // Process when audio file is changed
                audioFileElement.addEventListener("change", async function (event) {
                    audioInfoElement.textContent = "";
                    const input = event.target;
                    if (input.files.length === 0) {
                        return;
                    }
                    const selectedFile = input.files[0];
                    if (!(/\.(?:wav|mp3|flac|opus|m4a|mp4|webm)$/i.test(selectedFile.name))) {
                        return;
                    }
                    const audioInfo = await getAudioInfo(selectedFile);
                    if (audioInfo !== null) {
                        audioInfoElement.textContent = audioInfo;
                    }
                });

                /**
                 * Get audio file information.
                 * @param {File} audioFile Audio file
                 * @returns Audio file information
                 */
                async function getAudioInfo(audioFile) {
                    const getAudioInfo_ = function (audioFile) {
                        return new Promise((resolve, reject) => {
                            if (typeof MediaStreamTrackProcessor !== 'undefined') {
                                const videoElement = document.createElement("video");
                                videoElement.width = 0;
                                videoElement.height = 0;
                                videoElement.volume = 0.01;
                                videoElement.autoplay = true;
                                document.body.appendChild(videoElement);
                                videoElement.onplay = function () {
                                    videoElement.onplay = null;
                                    const stream = videoElement.captureStream();
                                    const audioTrack = stream.getAudioTracks()[0];
                                    const processor = new MediaStreamTrackProcessor({ track: audioTrack });
                                    const processorReader = processor.readable.getReader();
                                    processorReader.read().then(function (result) {
                                        if (result.done) {
                                            return;
                                        }
                                        videoElement.pause();
                                        videoElement.currentTime = 0;
                                        stream.getAudioTracks().forEach((track) => {
                                            track.stop();
                                        });
                                        try {
                                            processorReader.cancel();
                                        } catch (e) { }
                                        const audioDuration = videoElement.duration;
                                        URL.revokeObjectURL(videoElement.src);
                                        videoElement.src = "";
                                        document.body.removeChild(videoElement);
                                        resolve(
                                            audioFile.type + " "
                                            + result.value.sampleRate + "Hz "
                                            + result.value.numberOfChannels + "ch "
                                            + Math.floor(audioDuration) + "sec"
                                        );
                                    });
                                };
                                videoElement.src = URL.createObjectURL(audioFile);
                            } else {
                                resolve(null);
                            }
                        });
                    };
                    return await getAudioInfo_(audioFile);
                }

                                    // Execute Asynchronous HTTP Speech Recognition API
                executeAsyncButtonElement.addEventListener("click", function (event) {
                    if (appKeyElement.value.length === 0) {
                        alert("Please enter APPKEY.");
                        return;
                    }
                    if (audioFileElement.files.length === 0) {
                        alert("Please select an audio file.");
                        return;
                    }
                    const selectedFile = audioFileElement.files[0];
                    if (!(/\.(?:wav|mp3|flac|opus|m4a|mp4|webm)$/i.test(selectedFile.name))) {
                        alert("Please select a .wav, .mp3, .flac, .opus, .m4a, .mp4, or .webm file.");
                        return;
                    }
                    addLog("Starting job registration process.");
                    const asyncHrp = new AsyncHrp();
                    asyncHrp.onProgress = function (message, sessionId) {
                        addLog((sessionId !== null ? "[" + sessionId + "]" : "") + message);
                    };
                    asyncHrp.onError = function (message, sessionId) {
                        addLog((sessionId !== null ? "[" + sessionId + "]" : "") + message);
                    };
                    asyncHrp.onCompleted = function (resultJson, sessionId) {
                        addLog(resultJson.text);
                        drawResultView(resultJson, selectedFile);
                    };
                    asyncHrp.engineMode = engineModeElement.value;
                    asyncHrp.loggingOptOut = loggingOptOutElement.checked;
                    asyncHrp.keepFillerToken = keepFillerTokenElement.checked;
                    asyncHrp.speakerDiarization = speakerDiarizationElement.checked;
                    asyncHrp.sentimentAnalysis = sentimentAnalysisElement.checked;
                    asyncHrp.profileWords = profileWordsElement.value.trim();

                    postJob(appKeyElement.value, selectedFile, asyncHrp);
                });

                // Execute Synchronous HTTP Speech Recognition API
                executeHrpButtonElement.addEventListener("click", function (event) {
                    if (appKeyElement.value.length === 0) {
                        alert("Please enter APPKEY.");
                        return;
                    }
                    if (audioFileElement.files.length === 0) {
                        alert("Please select an audio file.");
                        return;
                    }
                    const selectedFile = audioFileElement.files[0];
                    if (!(/\.(?:wav|mp3|flac|opus|m4a|mp4|webm)$/i.test(selectedFile.name))) {
                        alert("Please select a .wav, .mp3, .flac, .opus, .m4a, .mp4, or .webm file.");
                        return;
                    }
                    addLog("Executing Synchronous HTTP Speech Recognition API.");
                    const easyHrp = new EasyHrp();
                    easyHrp.onError = function (message, sessionId) {
                        addLog((sessionId != null ? "[" + sessionId + "]" : "") + message);
                    };
                    easyHrp.onCompleted = function (resultJson, sessionId) {
                        addLog(resultJson.text);
                        drawResultView(resultJson, selectedFile);
                    };
                    easyHrp.engineMode = engineModeElement.value;
                    easyHrp.loggingOptOut = loggingOptOutElement.checked;
                    easyHrp.keepFillerToken = keepFillerTokenElement.checked;
                    easyHrp.speakerDiarization = speakerDiarizationElement.checked;
                    easyHrp.profileWords = profileWordsElement.value.trim();

                    postJob(appKeyElement.value, selectedFile, easyHrp);
                });

                /**
                 * Execute Asynchronous/Synchronous HTTP Speech Recognition API.
                 * @param {string} appKey APPKEY
                 * @param {File} audioFile Audio file
                 * @param {object} recognizerClient AsyncHrp/EasyHrp object
                 */
                function postJob(appKey, audioFile, recognizerClient) {
                    const reader = new FileReader();
                    reader.onload = () => {
                        const AudioContext = window.AudioContext || window.webkitAudioContext;
                        const audioContext = new AudioContext({ sampleRate: 16000 });
                        // Convert audio file to 32bit linear PCM
                        audioContext.decodeAudioData(reader.result, async function (audioBuffer) {
                            // Downmix to mono
                            const OfflineAudioContext = window.OfflineAudioContext || window.webkitOfflineAudioContext;
                            const offlineAudioContext = new OfflineAudioContext(audioBuffer.numberOfChannels, audioBuffer.length, audioBuffer.sampleRate);
                            const merger = offlineAudioContext.createChannelMerger(audioBuffer.numberOfChannels);
                            const source = offlineAudioContext.createBufferSource();
                            source.buffer = audioBuffer;

                            for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
                                source.connect(merger, 0, i);
                            }
                            merger.connect(offlineAudioContext.destination);
                            source.start();

                            const mixedBuffer = await offlineAudioContext.startRendering();
                            const float32PcmData = mixedBuffer.getChannelData(0);

                            merger.disconnect();
                            source.disconnect();
                            audioContext.close();

                            if (useOpusRecorderElement.checked) {
                                // Convert mono 32bit linear PCM to Ogg Opus
                                const opusEncoderWrapper = new OpusEncoderWrapper();
                                opusEncoderWrapper.originalSampleRate = audioBuffer.sampleRate;
                                opusEncoderWrapper.useStream = false;
                                opusEncoderWrapper.onCompleted = function (opusData) {
                                    const convertedAudioFile = new Blob([opusData], { type: "audio/ogg; codecs=opus" });
                                    // Execute speech recognition
                                    if (typeof recognizerClient.postJob !== 'undefined') {
                                        recognizerClient.postJob(appKey, convertedAudioFile);
                                    }
                                };
                                await opusEncoderWrapper.initialize();
                                opusEncoderWrapper.start();
                                opusEncoderWrapper.encode(float32PcmData);
                                opusEncoderWrapper.stop();
                            } else {
                                // Convert mono 32bit linear PCM to custom header DVI/IMA ADPCM
                                const audioFileConverter = new Worker('./scripts/ami-adpcm-worker.js');
                                audioFileConverter.onmessage = (event) => {
                                    const convertedAudioFile = new Blob([event.data], { type: "application/octet-stream" });
                                    audioFileConverter.terminate();
                                    // Execute speech recognition
                                    if (typeof recognizerClient.postJob !== 'undefined') {
                                        recognizerClient.postJob(appKey, convertedAudioFile);
                                    }
                                };
                                audioFileConverter.postMessage([float32PcmData, audioBuffer.sampleRate], [float32PcmData.buffer]);
                            }
                        }, () => {
                            addLog("Can't decode audio data.");
                        });
                    };
                    reader.readAsArrayBuffer(audioFile);
                }

                // Execute WebSocket Speech Recognition API
                startWrpButtonElement.addEventListener("click", function (event) {
                    if (Wrp.isActive()) {
                        return;
                    }
                    
                    // Initialize latency tracking WebSocket wrapper
                    createLatencyTrackingWebSocket();
                    
                    // Reset latency tracking for new session
                    lastAudioSentTime = null;
                    latencyHistory = [];
                    audioChunkCounter = 0;
                    resultCounter = 0;
                    updateLatencyInfo();
                    
                    let resultJson = [];
                    Wrp.serverURL = "wss://acp-api.amivoice.com/v1/";
                    if (loggingOptOutElement.checked) {
                        Wrp.serverURL += "nolog/";
                    }
                    Wrp.grammarFileNames = engineModeElement.value;
                    Wrp.authorization = appKeyElement.value;

                    Wrp.profileWords = profileWordsElement.value.trim();
                    Wrp.keepFillerToken = keepFillerTokenElement.checked ? 1 : 0;
                    Wrp.resultUpdatedInterval = 1000;
                    Wrp.checkIntervalTime = 600000;
                    Wrp.segmenterProperties = speakerDiarizationElement.checked ? "useDiarizer=1" : "";

                    Recorder.maxRecordingTime = 3600000;
                    Recorder.sampleRate = 16000;
                    Recorder.downSampling = true;
                    Recorder.adpcmPacking = true;
                    Recorder.useOpusRecorder = useOpusRecorderElement.checked;
                    Recorder.useUserMedia = useUserMediaElement.checked;
                    Recorder.useDisplayMedia = useDisplayMediaElement.checked;

                    Wrp.TRACE = function (message) {
                        if (message.startsWith("ERROR:")) {
                            addLog("WebSocket: " + message);
                        }
                    };
                    Wrp.connectStarted = function () {
                        addLog("WebSocket: Connecting to speech recognition server...");
                    };
                    Wrp.connectEnded = function () {
                        addLog("WebSocket: Speech recognition server connection completed (speech recognition ready).");
                    };
                    Wrp.disconnectStarted = function () {
                        addLog("WebSocket: Disconnecting from speech recognition server...");
                    };
                    Wrp.disconnectEnded = function () {
                        addLog("WebSocket: Speech recognition server disconnection completed.");
                    };
                    Wrp.resultUpdated = function (result) {
                        const resultJsonPart = JSON.parse(result);
                        addLog(resultJsonPart.text);
                        recordResultReceived();
                    };
                    Wrp.resultFinalized = function (result) {
                        const resultJsonPart = JSON.parse(result);
                        addLog(resultJsonPart.text);
                        resultJson.push(resultJsonPart);
                        recordResultReceived();
                    };
                    Wrp.feedDataPauseEnded = function () {
                        const blob = Recorder.getWaveFile();
                        if (!blob) {
                            return;
                        }
                        drawResultView(resultJson, blob);
                    };
                    Wrp.feedDataResume();
                });
                // Stop WebSocket Speech Recognition API
                stopWrpButtonElement.addEventListener("click", function (event) {
                    if (!Wrp.isActive()) {
                        return;
                    }
                    Wrp.feedDataPause();
                });

                /**
                 * Build screen (HTML elements) from recognition result JSON
                 * @param {object} resultJson Speech recognition result JSON
                 * @param {object} audioFile Audio file
                 */
                function drawResultView(resultJson, audioFile) {
                    const webVttConverter = new Worker('./scripts/ami-webvtt-worker.js');
                    webVttConverter.onmessage = (event) => {
                        const vttSamples = [];
                        vttSamples[0] = event.data;
                        webVttConverter.terminate();

                        // WEBVTT with additional information removed except readings (remove styles, sentiment analysis results, v tags)
                        vttSamples[1] = vttSamples[0].replace(/\nSTYLE[\s\S]+}\n/m, '')
                            .replaceAll(new RegExp('([0-9:.]+ --> [0-9:.]+)(?:.+)', "g"), '$1')
                            .replaceAll(/\n[0-9]+\n[0-9:.]+ --> [0-9:.]+\nENERGY:[0-9]{3} STRESS:[0-9]{3}\n/mg, '')
                            .replaceAll(/<\/?(?:v|[0-9])[^>]*>/g, '');

                        // WEBVTT with readings also removed
                        vttSamples[2] = vttSamples[0].replace(/\nSTYLE[\s\S]+}\n/m, '')
                            .replaceAll(new RegExp('([0-9:.]+ --> [0-9:.]+)(?:.+)', "g"), '$1')
                            .replaceAll(/\n[0-9]+\n[0-9:.]+ --> [0-9:.]+\nENERGY:[0-9]{3} STRESS:[0-9]{3}\n/mg, '')
                            .replaceAll(/<\/?(?:v|[0-9])[^>]*>/g, '')
                            .replaceAll(/<rt>[^<]*<\/rt>/g, '')
                            .replaceAll(/<[^>]*>/g, '');

                        // WEBVTT without removing readings, but adding spaces before and after readings where they exist
                        vttSamples[3] = vttSamples[0].replace(/\nSTYLE[\s\S]+}\n/m, '')
                            .replaceAll(new RegExp('([0-9:.]+ --> [0-9:.]+)(?:.+)', "g"), '$1')
                            .replaceAll(/\n[0-9]+\n[0-9:.]+ --> [0-9:.]+\nENERGY:[0-9]{3} STRESS:[0-9]{3}\n/mg, '')
                            .replaceAll(/<\/?(?:v|[0-9])[^>]*>/g, '')
                            .replaceAll(/<ruby>[^<]*<rt>([^<]*)<\/rt><\/ruby>/g, ' $1 ')
                            .replaceAll(/ {2,}/g, ' ')
                            .replaceAll(/(?: \n|\n )/mg, '\n')
                            .replaceAll(/<[^>]*>/g, '')
                            .replaceAll(/ ([,.?!])/g, '$1');

                        // Create links to open or download recognition result JSON in a separate window
                        const links = document.createElement("div");
                        const resultJsonUrl = URL.createObjectURL(
                            new Blob([JSON.stringify(resultJson, null, "\t")], { type: 'application/json;charset=utf-8' }));
                        links.appendChild(createDownloadLink(resultJsonUrl, "JSON", "result.json"));

                        // Create links to open or download recognition result JSON text in a separate window
                        const linkText = document.createElement("a");
                        let resultText = "";
                        if (typeof resultJson.segments !== 'undefined') {
                            // Get text separated by line breaks for each segment from asynchronous HTTP speech recognition API result JSON
                            resultText = resultJson.segments.map(segment => segment.results[0].text)
                                .filter(value => (typeof value !== 'undefined' && value.length > 0))
                                .join("\n") + "\n";
                        } else {
                            if (Array.isArray(resultJson)) {
                                // Get text separated by line breaks for each utterance from WebSocket speech recognition API result JSON array
                                resultText = resultJson.map(json => json.text)
                                    .filter(value => (typeof value !== 'undefined' && value.length > 0))
                                    .join("\n") + "\n";
                            } else {
                                // Get text from synchronous HTTP speech recognition API result JSON
                                resultText = resultJson.text + "\n";
                            }
                        }
                        const resultTextUrl = URL.createObjectURL(
                            new Blob([resultText], { type: 'text/plain;charset=utf-8' }));
                        links.appendChild(createDownloadLink(resultTextUrl, "TEXT", "result.txt"));

                        // Create video element for subtitle verification
                        const player = document.createElement("video");
                        player.src = URL.createObjectURL(audioFile);

                        // Set up video element subtitles and create WebVTT links
                        for (let i = 0; i < vttSamples.length; i++) {
                            const track = document.createElement("track");
                            if (i == 0) {
                                track.setAttribute('default', '');
                            }
                            const vttUrl = URL.createObjectURL(
                                new Blob([vttSamples[i]], { type: 'text/vtt;charset=utf-8' }));
                            track.src = vttUrl;
                            track.label = "Sample" + (i + 1).toString();
                            player.appendChild(track);
                            links.appendChild(createDownloadLink(
                                vttUrl, "WebVTT Sample" + (i + 1).toString(), "sample" + (i + 1).toString() + ".vtt"));
                        }
                        player.addEventListener("mouseover", function () {
                            this.setAttribute("controls", "");
                        });
                        player.addEventListener("mouseout", function () {
                            this.removeAttribute("controls");
                        });
                        document.body.appendChild(links);
                        document.body.appendChild(player);
                    };
                    webVttConverter.postMessage(resultJson);
                }

                /**
                 * Create download link.
                 * @param {string} url URL
                 * @param {string} title Title
                 * @param {string} fineName File name
                 * @returns HTML element
                 */
                function createDownloadLink(url, title, fileName) {
                    const divElement = document.createElement("div");

                    const openLink = document.createElement("a");
                    openLink.href = url;
                    openLink.target = "_blank";
                    openLink.rel = "noopener noreferrer";
                    openLink.textContent = "Open " + title;
                    divElement.appendChild(openLink);

                    const downloadLink = document.createElement("a");
                    downloadLink.href = url;
                    downloadLink.textContent = "Download";
                    downloadLink.title = "Download " + title;
                    downloadLink.download = fileName;
                    divElement.appendChild(downloadLink);

                    return divElement;
                }

                /**
                 * Output log.
                 * @param {string} log Log string
                 */
                function addLog(log) {
                    logsElement.textContent += (new Date().toISOString() + " " + log + "\n");
                    setTimeout(function () { logsElement.scrollTop = logsElement.scrollHeight; }, 200);
                }
            })();
        </script>
</body>

</html>